{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The textbook formula for an OLS regression is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (X^{'}X)^{-1}X^{'}Y\n",
    "\\end{equation}\n",
    "\n",
    "Where X are the independent variables and Y is the dependent variable.\n",
    "\n",
    "The textbok formula for an IV regression is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (X^{'}ZWZ^{'}X)^{-1} X^{'}ZWZ^{'}Y\n",
    "\\end{equation}\n",
    "\n",
    "Where again, X are the independent variables and Y is the dependent variable. Moreover, Z are the instruments (including the X variables that don't need and instrument) and W is a weighting matrix. \n",
    "\n",
    "The usual choice for the weighting matrix is:\n",
    "\n",
    "\\begin{equation}\n",
    "W = (\\frac{1}{n} Z^{'}Z)^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "Which makes the above IV formula equivalent to a two staged least squares approach.\n",
    "\n",
    "There is code for all three equations in `ols.py` and `iv.py`. I did not try to run that code and typed it very quickly, so it is possible that it contains bugs.\n",
    "\n",
    "All of these equations contain the inverse of a matrix. While this leads to a concise and interpretable equation, it is rarely a good idea to actually implement it that way. Here is a [blogpost](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/) on the topic. In essence: using the inverse is both slow and inaccurate, so it should never be done.\n",
    "\n",
    "There is a [stackoverflow page](https://stackoverflow.com/questions/41648246/efficient-computation-of-the-least-squares-algorithm-in-numpy) that describes some alternatives and a [package](https://gist.github.com/aldro61/5889795) that implements a fast solution algorithm. [Statsmodels](https://github.com/statsmodels/statsmodels/blob/master/statsmodels/regression/linear_model.py#L227) also has some code to fit a least squares regression based on a qr decomposition. \n",
    "\n",
    "As becomes apparent from the stackoverflow page, in general there is a trade-off between accuracy and speed, which is especially relevant when there is a colinearity problem.  \n",
    "\n",
    "## Remarks\n",
    "\n",
    "- whenever you generate random numbers, set a seed (np.random.seed) to make your results reproducible\n",
    "- the results of this task will probably presented in one of the next hackathons. I would prefer that you do the presentation, but if you don't want to, I can do it. \n",
    "\n",
    "## Tasks\n",
    "\n",
    "0. Check the formulae against some standard textbook!\n",
    "\n",
    "1. Create several benchmark datasets for X, Y and Z (in this notebook). The different datasets should have different numbers of variables and instruments (from 5 to 50) as well as different numbers of observations (from 300 to 20 000). For all datasets you should know the true beta. Some datatsets should be well behaved. Others should have one of the following problems:\n",
    "    - strong (or even very strong) colinearity\n",
    "    - weak instruments\n",
    "    - X and Z variables of very different orders of magnitudes (e.g one X variable that varies only between 0 and 1 and another that varies between - 10 000 and + 10 000\n",
    "        \n",
    "2. Write better implementations for the three functions I gave you. You can take the code samples from the links, google on your own or look for books on the topic. Try out several implementations for each function! For OLS at least parts of the work have already been done by Tobias Raabe. You can find his notebook in this repo.\n",
    "\n",
    "3. Import your functions into this notebook and time the different implementations using %timeit. I have a code snippet below that shows how. Make the following plots. (For all plots you should plot the results of all implementations into the same figure).:\n",
    "    - execution time on the y-axis and number of observations on the x-axis with a fix number of variables (say 15) and well behaved datasets. You don't have to be as fine grained as in the assignment, i.e. can make larger steps in the sample size. \n",
    "    - execution time on the y-axis and number of x-variables on the x-axis, with a fix sample size (say 3000 observations).\n",
    "    - execution time on the y-axis and number of instruments (10 to 30) on the x-axis, with a sample size of 3000 and 10 x-variables.   \n",
    "    \n",
    "4. Now test the accurracy of the different implementations. Your measure of accuracy should be the root mean squared error (rmse) between the true beta and the estimated beta. Make plots that show how the accuracy deteriorates when the x-matrix is almost not invertible (due to colinearity) and when a instrument becomes very weak. To get enough precision in the rmse, you'll probably have to average the rmse across several runs with datasets that are equal except for the realizations of the random components.\n",
    "\n",
    "5. Integrate the best implementation for IV into skillmodels and the best implementation for OLS into respy. This is only after we discussed the results to the previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.8 ns ± 2.52 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Some code snippets that show how to use timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def some_function():\n",
    "    return 1 + 1\n",
    "\n",
    "time_result = %timeit -o some_function()\n",
    "average_time = np.mean(time_result.timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
